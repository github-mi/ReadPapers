# Optimal Completion Distillation for Sequence Learning(OCD)

这篇论文是Google的工作，作者是Sara Sabour。

在2018年10月这篇论文刚出来的时候在LibriSpeech上获得了state-of-the-art的结果，WER在test-clean测试集上为4.5%，在test-other测试集上13.3%。不过半年之后就被[SpecAugment](https://arxiv.org/abs/1904.08779)给干趴下了，test-clean上做到2.8%，test-other上做到6.8%。

这篇论文针对传统ED存在的问题做了改进，传统ED存在两处mismatch的地方：

1. 训练时给的golden的 $y_{t-1}$ ，而解码时使用的是预测出来的 $y_{t-1}$ 。
2. 训练时一般使用celoss，而评估准则一般是编辑距离(Edit Distance)。

OCD的做法是不在ground truth序列上训练，而是在采样出来的序列上进行训练，在给定 $\hat{y}_{<t}$ 时，预测的目标是所有可能的后缀中使得与ground truth编辑距离最小的序列的第一个token(有多个最优后缀时，均分目标)。示例如下：

<img src="E:\Github\ReadPapers\ASR\png\OCD_Tabel1.png" alt="1569323616743" style="zoom:80%;" />

在LibriSpeech上的实验结果见下表，test-clean的WER相对提升21%，test-other的WER相对提升13.6%，是当时state-of-the-art的结果。

<img src="E:\Github\ReadPapers\ASR\png\OCD_Tabel5.png" alt="1569324182554" style="zoom:80%;" />

在复现这篇论文的工作时发现，只使用OCD的loss没啥效果，celoss和OCD loss一起使用时才会有提升。

# Read List

- [x] [Optimal Completion Distillation for Sequence Learning](https://arxiv.org/abs/1810.01398)
- [ ] [Posterior Attention Models for Sequence to Sequence Learning](https://openreview.net/forum?id=BkltNhC9FX)

